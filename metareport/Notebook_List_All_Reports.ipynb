{"cells":[{"cell_type":"markdown","source":["## Initialization"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jp-MarkdownHeadingCollapsed":true},"id":"1216236c-848a-403b-ba9d-660b02836de1"},{"cell_type":"markdown","source":["### Get token"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jp-MarkdownHeadingCollapsed":true},"id":"60823411-7c86-4134-8d46-bc28f8db846d"},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# Setup: Define Power BI API Access and Workspace Context\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","from notebookutils import mssparkutils\n","import requests, time, os\n","\n","# Power BI REST API base URL\n","api = \"https://api.powerbi.com/v1.0/myorg\"\n","\n","# List of target workspace (group) IDs to query\n","workspace_ids = [\n","    \"5b129136-***\",\n","    \"00fc8160-***\"\n","]\n","\n","# Acquire Azure AD token for Power BI REST API using Microsoft Fabric credentials\n","token = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n","\n","# Construct authorization header for authenticated API calls\n","headers = {\n","    \"Authorization\": f\"Bearer {token}\",\n","    \"Content-Type\": \"application/json\"\n","}\n","\n","# Confirm authentication success\n","print(\"âœ… Complete. Token received.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bfba7dec-6de3-482c-abae-074912fc74e2"},{"cell_type":"markdown","source":["### List all reports in the workspaces"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jp-MarkdownHeadingCollapsed":true},"id":"a17690d6-2fc4-4fd1-b68e-34ad4857a24b"},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# Data Retrieval: Retrieve All Power BI Reports by Workspace\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","import requests\n","\n","all_reports = []  # Will collect all reports across workspaces\n","\n","for ws_id in workspace_ids:\n","    # Retrieve reports from the Power BI REST API for each workspace\n","    resp = requests.get(\n","        f\"{api}/groups/{ws_id}/reports\",\n","        headers=headers\n","    )\n","\n","    if not resp.ok:\n","        # Fail-fast on API error with clear context for debugging\n","        raise RuntimeError(\n","            f\"âŒ Failed to retrieve reports for workspace {ws_id} \"\n","            f\"(HTTP {resp.status_code}): {resp.text}\"\n","        )\n","\n","    # Extract the list of reports or default to empty\n","    reports = resp.json().get(\"value\", [])\n","\n","    for r in reports:\n","        # Enrich each report with workspace ID for downstream traceability\n","        r[\"workspaceId\"] = ws_id\n","        all_reports.append(r)\n","\n","# Final result collection\n","reports = all_reports\n","\n","# Log total number of reports retrieved\n","print(f\"âœ… Complete. Total reports loaded: {len(all_reports)}\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"96aa370c-81a8-4e47-8214-001cd9adcab5"},{"cell_type":"markdown","source":["## Prepare images"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jp-MarkdownHeadingCollapsed":true},"id":"5af8cf8c-5051-48c9-846f-e63764df2944"},{"cell_type":"markdown","source":["### Export PNG images (first visible page of each report)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jp-MarkdownHeadingCollapsed":true},"id":"be92b10b-254d-44a6-a437-acade17a1de1"},{"cell_type":"code","source":["import requests\n","import time\n","import os\n","from pyspark.sql import Row\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# âš™ï¸ Export Timeout Configuration\n","# Define timeout and polling interval for Power BI export jobs.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","MAX_EXPORT_SECONDS = 30\n","POLL_INTERVAL_SECONDS = 5\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ§¹ Cleanup: Remove Existing PNG Files\n","# Ensures stale thumbnail files are not mixed with current run results.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","thumbs_dir = \"Files/ReportThumbnails\"\n","\n","if mssparkutils.fs.exists(thumbs_dir):\n","    mssparkutils.fs.rm(thumbs_dir, recurse=True)\n","    print(f\"ğŸ§¹ Removed old thumbnails from '{thumbs_dir}'\")\n","\n","mssparkutils.fs.mkdirs(thumbs_dir)\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“¤ Export Logic: Power BI Report Thumbnails (with timeout)\n","# Exports first visible page of each report as a PNG file.\n","# Retries are not implemented; failures are skipped with warnings.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def export_default_page_png(report_id: str, report_name: str) -> dict | None:\n","    \"\"\"\n","    Export first visible page of a Power BI report to PNG.\n","    Aborts if export job does not complete in time.\n","    Returns file metadata if successful, else None.\n","    \"\"\"\n","\n","    # â”€â”€ Step 1: Fetch pages for the report\n","    resp = requests.get(f\"{api}/reports/{report_id}/pages\", headers=headers)\n","\n","    if not resp.ok:\n","        raise RuntimeError(\n","            f\"âŒ Failed to retrieve pages for report '{report_name}' \"\n","            f\"(HTTP {resp.status_code}): {resp.text}\"\n","        )\n","\n","    pages = resp.json().get(\"value\", [])\n","    visible_pages = sorted(\n","        (p for p in pages if p.get(\"visibility\", \"Visible\") == \"Visible\"),\n","        key=lambda p: p.get(\"order\", 0)\n","    )\n","\n","    if not visible_pages:\n","        raise RuntimeError(f\"âŒ No visible pages found for report '{report_name}'\")\n","\n","    page = visible_pages[0]\n","    page_name = page[\"name\"]\n","    page_display_name = page.get(\"displayName\", page_name)\n","\n","    print(f\"ğŸ“„ Exporting page â†’ '{page_display_name}' ({page_name})\")\n","\n","    # â”€â”€ Step 2: Start export job\n","    export_resp = requests.post(\n","        f\"{api}/reports/{report_id}/ExportTo\",\n","        headers=headers,\n","        json={\n","            \"format\": \"PNG\",\n","            \"powerBIReportConfiguration\": {\n","                \"pages\": [{\"pageName\": page_name}]\n","            }\n","        }\n","    )\n","\n","    if not export_resp.ok:\n","        raise RuntimeError(\n","            f\"âŒ Failed to start export for report '{report_name}' \"\n","            f\"(HTTP {export_resp.status_code}): {export_resp.text}\"\n","        )\n","\n","    export_id = export_resp.json().get(\"id\")\n","    start_ts = time.time()\n","\n","    # â”€â”€ Step 3: Poll until export succeeds, fails, or times out\n","    while True:\n","        status_resp = requests.get(\n","            f\"{api}/reports/{report_id}/exports/{export_id}\",\n","            headers=headers\n","        )\n","\n","        if not status_resp.ok:\n","            raise RuntimeError(\n","                f\"âŒ Failed to poll export status for report '{report_name}' \"\n","                f\"(HTTP {status_resp.status_code}): {status_resp.text}\"\n","            )\n","\n","        status = status_resp.json().get(\"status\")\n","\n","        if status == \"Succeeded\":\n","            break\n","        if status in (\"Failed\", \"Cancelled\"):\n","            print(f\"âŒ Export {status} for report '{report_name}' â€” skipped\")\n","            return None\n","        if time.time() - start_ts > MAX_EXPORT_SECONDS:\n","            print(f\"âš ï¸ Export timed out after {MAX_EXPORT_SECONDS}s for report '{report_name}' â€” skipped\")\n","            return None\n","\n","        time.sleep(POLL_INTERVAL_SECONDS)\n","\n","    # â”€â”€ Step 4: Download exported PNG file and move to Lakehouse\n","    file_bytes = requests.get(\n","        f\"{api}/reports/{report_id}/exports/{export_id}/file\",\n","        headers=headers\n","    ).content\n","\n","    filename = f\"{report_id}.png\"\n","    local_path = f\"/tmp/{filename}\"\n","\n","    with open(local_path, \"wb\") as f:\n","        f.write(file_bytes)\n","\n","    lakehouse_path = f\"{thumbs_dir}/{filename}\"\n","    mssparkutils.fs.mv(\n","        f\"file:{local_path}\",\n","        lakehouse_path,\n","        overwrite=True\n","    )\n","\n","    print(f\"ğŸ’¾ Saved â†’ {lakehouse_path} ({len(file_bytes) / 1024:.1f} KB)\")\n","\n","    return {\n","        \"ReportId\": report_id,\n","        \"ReportName\": report_name,\n","        \"FilePath\": f\"/lakehouse/default/{lakehouse_path}\",\n","        \"FileSizeKB\": round(len(file_bytes) / 1024, 1)\n","    }\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ” Process Reports (skip failed exports)\n","# Attempt PNG export for each report and collect metadata.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","meta_rows = []\n","\n","for rep in reports:\n","    print(f\"ğŸ“¤ Processing report '{rep['name']}' â€¦\")\n","    result = export_default_page_png(rep[\"id\"], rep[\"name\"])\n","    if result:\n","        meta_rows.append(Row(**result))\n","    else:\n","        print(f\"âš ï¸ Thumbnail not created for report '{rep['name']}'\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“ Metadata Table\n","# Write metadata about exported PNGs to Delta table.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_meta = spark.createDataFrame(meta_rows)\n","\n","df_meta.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"ReportThumbnailsMeta\")\n","\n","print(\"âœ… Complete. Table 'ReportThumbnailsMeta' created\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7798b2f9-577a-4424-8e78-cb82a79476f4"},{"cell_type":"markdown","source":["### Resize PNGs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"102ee4ee-a010-4524-ac1b-ee58c7decefd"},{"cell_type":"code","source":["from PIL import Image\n","import io\n","import os\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“ Resize PNGs to Max 500px (Fabric-Safe, Binary-Preserving)\n","# Resize thumbnails if their largest side exceeds 500px.\n","# Results are safely overwritten in Lakehouse Files directory.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","# Read all PNG files from the thumbnails directory as binary content\n","df_images = (\n","    spark.read.format(\"binaryFile\")\n","    .option(\"pathGlobFilter\", \"*.png\")\n","    .load(\"Files/ReportThumbnails\")\n","    .select(\"path\", \"content\")\n",")\n","\n","# âš ï¸ Safe to collect â€” small number of thumbnails expected\n","rows = df_images.collect()\n","\n","for row in rows:\n","    full_path = row[\"path\"]\n","    content = bytes(row[\"content\"])  # raw PNG bytes from Lakehouse\n","\n","    # â”€â”€ Open image in-memory using PIL\n","    img = Image.open(io.BytesIO(content))\n","    max_side = max(img.size)\n","\n","    if max_side <= 500:\n","        # Skip if image is already within size constraints\n","        print(f\"âœ… Already within limit â†’ {full_path}\")\n","        continue\n","\n","    # â”€â”€ Resize image while preserving aspect ratio\n","    scale = 500 / max_side\n","    new_size = (\n","        int(img.size[0] * scale),\n","        int(img.size[1] * scale)\n","    )\n","\n","    img = img.resize(new_size, Image.LANCZOS)\n","\n","    # â”€â”€ Save resized image to local /tmp path\n","    filename = os.path.basename(full_path)\n","    local_path = f\"/tmp/{filename}\"\n","\n","    with open(local_path, \"wb\") as f:\n","        img.save(f, format=\"PNG\")\n","\n","    # â”€â”€ Resolve Lakehouse-relative path from full Spark path\n","    files_path = \"Files/\" + full_path.split(\"/Files/\")[1]\n","\n","    # â”€â”€ Move resized PNG back into Lakehouse (overwrite original)\n","    mssparkutils.fs.mv(\n","        f\"file:{local_path}\",\n","        files_path,\n","        overwrite=True\n","    )\n","\n","    print(f\"ğŸ“ Resized â†’ {files_path} ({new_size[0]}x{new_size[1]})\")\n","\n","print(\"âœ… Complete\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"117fcae2-816a-4fd0-90b0-76a73a2e14f1"},{"cell_type":"markdown","source":["## Save into the Lakehouse table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"782133d4-2106-4cf1-917b-4b6fe1a0e206"},{"cell_type":"markdown","source":["### Get metadata, save into Delta table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dcc93d52-60ba-476d-8350-b8d691000c87"},{"cell_type":"code","source":["import requests\n","from pyspark.sql import Row, types as T\n","from datetime import datetime\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“„ Report + Refresh + Workspace Metadata (Explicit Schema, No Admin API)\n","# Collects report-level metadata and last refresh status for owned datasets.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","meta_schema = T.StructType([\n","    T.StructField(\"ReportId\", T.StringType(), False),\n","    T.StructField(\"ReportName\", T.StringType(), True),\n","    T.StructField(\"DatasetId\", T.StringType(), True),\n","    T.StructField(\"WorkspaceId\", T.StringType(), True),\n","    T.StructField(\"WorkspaceName\", T.StringType(), True),\n","    T.StructField(\"WebUrl\", T.StringType(), True),\n","    T.StructField(\"IsOwnedByMe\", T.BooleanType(), True),\n","\n","    # ğŸ” Dataset refresh metadata\n","    T.StructField(\"LastRefreshStatus\", T.StringType(), True),\n","    T.StructField(\"LastRefreshType\", T.StringType(), True),\n","    T.StructField(\"LastRefreshError\", T.StringType(), True),\n","\n","    # ğŸ•’ TRUE timestamp columns\n","    T.StructField(\"CreatedDateTime\", T.TimestampType(), True),\n","    T.StructField(\"ModifiedDateTime\", T.TimestampType(), True),\n","    T.StructField(\"LastRefreshStartTime\", T.TimestampType(), True),\n","    T.StructField(\"LastRefreshEndTime\", T.TimestampType(), True),\n","])\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ›¡ï¸ ISO 8601 â†’ Python datetime parser (REQUIRED)\n","# Handles timestamp conversion for Power BI API fields.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def parse_iso_timestamp(val):\n","    \"\"\"Convert ISO 8601 string to datetime. Return None if invalid.\"\"\"\n","    if isinstance(val, str):\n","        try:\n","            return datetime.fromisoformat(val.replace(\"Z\", \"+00:00\"))\n","        except ValueError:\n","            return None\n","    return None\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ¢ Resolve Workspace Names ONCE\n","# Avoids repeated API calls by caching workspace display names.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","workspace_names = {}\n","workspace_ids = {r[\"workspaceId\"] for r in reports if r.get(\"workspaceId\")}\n","\n","for ws_id in workspace_ids:\n","    resp = requests.get(f\"{api}/groups/{ws_id}\", headers=headers)\n","    workspace_names[ws_id] = resp.json().get(\"name\") if resp.ok else None\n","\n","print(f\"ğŸ¢ Workspace names resolved: {len(workspace_names)}\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ”„ Build Metadata Rows (STRICT TYPE CONTROL)\n","# Collects refresh status and report info using per-report API calls.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","meta_rows = []\n","\n","for rep in reports:\n","    report_id = rep[\"id\"]\n","    report_name = rep[\"name\"]\n","    workspace_id = rep.get(\"workspaceId\")\n","\n","    resp = requests.get(f\"{api}/reports/{report_id}\", headers=headers)\n","    if not resp.ok:\n","        raise RuntimeError(\n","            f\"âŒ Failed to retrieve metadata for report '{report_name}' \"\n","            f\"(HTTP {resp.status_code}): {resp.text}\"\n","        )\n","\n","    details = resp.json()\n","    dataset_id = details.get(\"datasetId\")\n","\n","    # Defaults\n","    refresh_start = refresh_end = refresh_status = refresh_type = refresh_error = None\n","\n","    if dataset_id:\n","        refresh_resp = requests.get(\n","            f\"{api}/datasets/{dataset_id}/refreshes\",\n","            headers=headers,\n","            params={\"$top\": 1}\n","        )\n","\n","        if refresh_resp.ok:\n","            refreshes = refresh_resp.json().get(\"value\", [])\n","            if refreshes:\n","                r = refreshes[0]\n","                refresh_start = parse_iso_timestamp(r.get(\"startTime\"))\n","                refresh_end = parse_iso_timestamp(r.get(\"endTime\"))\n","                refresh_status = r.get(\"status\")\n","                refresh_type = r.get(\"refreshType\")\n","                refresh_error = r.get(\"serviceExceptionJson\")\n","\n","    meta_rows.append(Row(\n","        ReportId=report_id,\n","        ReportName=report_name,\n","        DatasetId=dataset_id,\n","        WorkspaceId=workspace_id,\n","        WorkspaceName=workspace_names.get(workspace_id),\n","        WebUrl=details.get(\"webUrl\"),\n","        IsOwnedByMe=details.get(\"isOwnedByMe\"),\n","\n","        LastRefreshStatus=refresh_status,\n","        LastRefreshType=refresh_type,\n","        LastRefreshError=refresh_error,\n","\n","        CreatedDateTime=parse_iso_timestamp(details.get(\"createdDateTime\")),\n","        ModifiedDateTime=parse_iso_timestamp(details.get(\"modifiedDateTime\")),\n","        LastRefreshStartTime=refresh_start,\n","        LastRefreshEndTime=refresh_end,\n","    ))\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ’¾ Persist Metadata (NO CASTING NEEDED)\n","# Saves enriched metadata as Delta table for downstream use.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_meta = spark.createDataFrame(meta_rows, schema=meta_schema)\n","\n","df_meta.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"ReportThumbnailsMeta\")\n","\n","print(\"âœ… ReportThumbnailsMeta written\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d30ec5f-b52f-4950-850b-e7a10fa6ed4f"},{"cell_type":"markdown","source":["### Add base64 chunks of image data to the table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e34ee532-afd6-4c93-81d5-5a479919325c"},{"cell_type":"code","source":["import base64\n","from pyspark.sql import functions as F, types as T\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# âš™ï¸ Configuration\n","# Define the fixed number of base64 chunks and chunk size (in characters).\n","# Each image will be split into exactly 100 parts (padded or truncated).\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","CHUNK_SIZE = 8000\n","TOTAL_CHUNKS = 100\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ”§ UDF: Binary â†’ Base64 â†’ Fixed 100 Chunks\n","# Converts PNG binary to base64 string and slices it into uniform chunks.\n","# Ensures schema stability for downstream joins.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","def bin_to_fixed_chunks(b):\n","    if b is None:\n","        return [\"\"] * TOTAL_CHUNKS\n","\n","    b64 = base64.b64encode(b).decode(\"utf-8\")\n","\n","    chunks = [\n","        b64[i:i + CHUNK_SIZE]\n","        for i in range(0, len(b64), CHUNK_SIZE)\n","    ]\n","\n","    if len(chunks) < TOTAL_CHUNKS:\n","        chunks.extend([\"\"] * (TOTAL_CHUNKS - len(chunks)))\n","    else:\n","        chunks = chunks[:TOTAL_CHUNKS]\n","\n","    return chunks\n","\n","chunk_udf = F.udf(\n","    bin_to_fixed_chunks,\n","    T.ArrayType(T.StringType())\n",")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“¥ Load Existing Metadata Table\n","# Drops any existing chunk columns to maintain idempotency.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_meta = spark.table(\"ReportThumbnailsMeta\")\n","\n","chunk_cols = [f\"Chunk{i}\" for i in range(1, TOTAL_CHUNKS + 1)]\n","existing_chunk_cols = [c for c in chunk_cols if c in df_meta.columns]\n","\n","if existing_chunk_cols:\n","    print(f\"ğŸ§¹ Dropping existing chunk columns: {len(existing_chunk_cols)}\")\n","    df_meta = df_meta.drop(*existing_chunk_cols)\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ“¥ Read PNG Files (Source of Truth)\n","# Loads all PNGs as binary and extracts ReportId from filename.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_files = (\n","    spark.read.format(\"binaryFile\")\n","    .option(\"pathGlobFilter\", \"*.png\")\n","    .load(\"Files/ReportThumbnails\")\n","    .select(\n","        F.col(\"path\"),\n","        F.col(\"content\").alias(\"ThumbnailBinary\")\n","    )\n",")\n","\n","df_files = df_files.withColumn(\n","    \"ReportId\",\n","    F.regexp_extract(\"path\", r\"/([^/]+)\\.png$\", 1)\n",")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ”„ Convert Binary â†’ Base64 Chunks\n","# Uses UDF to convert each image to 100 base64-encoded string segments.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_chunks = df_files.withColumn(\n","    \"Base64Chunks\",\n","    chunk_udf(F.col(\"ThumbnailBinary\"))\n",")\n","\n","for i in range(TOTAL_CHUNKS):\n","    df_chunks = df_chunks.withColumn(\n","        f\"Chunk{i + 1}\",\n","        F.col(\"Base64Chunks\").getItem(i)\n","    )\n","\n","df_chunks = df_chunks.drop(\"ThumbnailBinary\", \"Base64Chunks\", \"path\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ”— Join Chunks Back to Metadata\n","# Aligns enriched binary columns with report metadata on ReportId.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_final = df_meta.join(df_chunks, on=\"ReportId\", how=\"left\")\n","\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ’¾ Overwrite Existing Table (Expanded Schema)\n","# Safe overwrite of full metadata table with base64 chunks.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","df_final.write \\\n","    .mode(\"overwrite\") \\\n","    .option(\"overwriteSchema\", \"true\") \\\n","    .format(\"delta\") \\\n","    .saveAsTable(\"ReportThumbnailsMeta\")\n","\n","print(\"âœ… Complete. Base64 chunks (Chunk1â€“Chunk100) refreshed in ReportThumbnailsMeta\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5f764312-95a8-4125-bab6-2a356546fd81"},{"cell_type":"markdown","source":["## Final cells"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2bd7e435-a4b9-43f7-8403-a5c012638a58"},{"cell_type":"markdown","source":["### Test and refresh"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"14a44092-fd4f-4bb4-bbaa-9460a7a6c880"},{"cell_type":"code","source":["# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","# ğŸ§¾ Post-Write Verification: Table Status & Optimization\n","# Check active database, validate schema, refresh metadata, and compute stats.\n","# Ensures downstream Spark performance and visibility in Fabric UI.\n","# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n","\n","# âœ… Confirm which database is active (default unless explicitly changed)\n","print(\"ğŸ“‚ Current database:\", spark.catalog.currentDatabase())\n","\n","# âœ… Review final schema for sanity check (after write)\n","spark.table(\"ReportThumbnailsMeta\").printSchema()\n","\n","# ğŸ” Force metadata refresh to ensure visibility in notebook UI + Fabric\n","spark.sql(\"REFRESH TABLE ReportThumbnailsMeta\")\n","\n","# âš™ï¸ Compute statistics to improve query planning and performance\n","spark.sql(\"ANALYZE TABLE ReportThumbnailsMeta COMPUTE STATISTICS\")\n","\n","print(\"âœ… Table metadata refreshed and statistics computed\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1cf76a86-785a-442d-9e18-01f5b0ad3a78"},{"cell_type":"markdown","source":["### Possible improvements:\n","\n","split the table in 2 tables: metadata, images\n","\n","split the notebbook in 2 notebooks (for metadata, and images), use different schedules (e.g., refresh metadata daily, images - weekly)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c4130ef4-f8bb-4fa7-baad-88ff0268398c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"29cba3da-ebac-46b3-b295-16b3d8758806"}],"default_lakehouse":"29cba3da-ebac-46b3-b295-16b3d8758806","default_lakehouse_name":"Lakehouse","default_lakehouse_workspace_id":"5b129136-0c96-4457-88a3-89ed2ce65b04"}}},"nbformat":4,"nbformat_minor":5}
