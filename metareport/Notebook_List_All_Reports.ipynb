{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1216236c-848a-403b-ba9d-660b02836de1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60823411-7c86-4134-8d46-bc28f8db846d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba7dec-6de3-482c-abae-074912fc74e2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": []
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Setup: Define Power BI API Access and Workspace Context\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from notebookutils import mssparkutils\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from pyspark.sql import Row, functions as F, types as T\n",
    "\n",
    "# Power BI REST API base URL\n",
    "api = \"https://api.powerbi.com/v1.0/myorg\"\n",
    "\n",
    "# Acquire Azure AD token for Power BI REST API using Microsoft Fabric credentials\n",
    "token = mssparkutils.credentials.getToken(\"https://analysis.windows.net/powerbi/api\")\n",
    "\n",
    "# Construct authorization header for authenticated API calls\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Confirm authentication success\n",
    "print(\"âœ… Complete. Token received.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17690d6-2fc4-4fd1-b68e-34ad4857a24b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### List all reports in the workspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff1414a-18e3-452a-9587-8b0dd27133dd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“‹ Discovery: List All Accessible Power BI Workspaces (ID + Name)\n",
    "# Useful for validating which workspaces are available via API\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "resp = requests.get(f\"{api}/groups\", headers=headers)\n",
    "\n",
    "if not resp.ok:\n",
    "    raise RuntimeError(\n",
    "        f\"âŒ Failed to retrieve workspaces (HTTP {resp.status_code}): {resp.text}\"\n",
    "    )\n",
    "\n",
    "groups = resp.json().get(\"value\", [])\n",
    "\n",
    "# Print workspace names and IDs\n",
    "print(\"ğŸ“‹ Available Workspaces:\")\n",
    "for g in groups:\n",
    "    print(f\" - {g['name']} â†’ {g['id']}\")\n",
    "\n",
    "# Extract and display full ID list for copy-paste use\n",
    "all_workspace_ids = [g[\"id\"] for g in groups]\n",
    "print(\"\\nğŸ§¾ All Workspace IDs:\")\n",
    "print(all_workspace_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa370c-81a8-4e47-8214-001cd9adcab5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Data Retrieval: Retrieve All Power BI Reports by Workspace\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# List of target workspace (group) IDs to query\n",
    "# Use the output of the above cell to get a list of IDs\n",
    "# Here you can choose what IDs should be included\n",
    "# TODO: Update these IDs based on your environment or use all_workspace_ids from the previous cell\n",
    "workspace_ids = [\n",
    "    \"5b129136-0c96-4457-88a3-89ed2ce65b04\",\n",
    "    \"00fc8160-a400-41d8-b1eb-e012126808c6\"\n",
    "]\n",
    "\n",
    "all_reports = []  # Will collect all reports across workspaces\n",
    "\n",
    "for ws_id in workspace_ids:\n",
    "    # Retrieve reports from the Power BI REST API for each workspace\n",
    "    resp = requests.get(\n",
    "        f\"{api}/groups/{ws_id}/reports\",\n",
    "        headers=headers\n",
    "    )\n",
    "\n",
    "    if not resp.ok:\n",
    "        # Fail-fast on API error with clear context for debugging\n",
    "        raise RuntimeError(\n",
    "            f\"âŒ Failed to retrieve reports for workspace {ws_id} \"\n",
    "            f\"(HTTP {resp.status_code}): {resp.text}\"\n",
    "        )\n",
    "\n",
    "    # Extract the list of reports or default to empty\n",
    "    reports = resp.json().get(\"value\", [])\n",
    "\n",
    "    for r in reports:\n",
    "        # Enrich each report with workspace ID for downstream traceability\n",
    "        r[\"workspaceId\"] = ws_id\n",
    "        all_reports.append(r)\n",
    "\n",
    "# Final result collection\n",
    "# Log total number of reports retrieved\n",
    "print(f\"âœ… Complete. Total reports loaded: {len(all_reports)}\")\n",
    "\n",
    "# Make reports available for subsequent cells\n",
    "reports = all_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8cf8c-5051-48c9-846f-e63764df2944",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Prepare images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92b10b-254d-44a6-a437-acade17a1de1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Export PNG images (first visible page of each report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798b2f9-577a-4424-8e78-cb82a79476f4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš™ï¸ Export Timeout Configuration\n",
    "# Define timeout and polling interval for Power BI export jobs.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "MAX_EXPORT_SECONDS = 30\n",
    "POLL_INTERVAL_SECONDS = 5\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§¹ Cleanup: Remove Existing PNG Files\n",
    "# Ensures stale thumbnail files are not mixed with current run results.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "thumbs_dir = \"Files/ReportThumbnails\"\n",
    "\n",
    "if mssparkutils.fs.exists(thumbs_dir):\n",
    "    mssparkutils.fs.rm(thumbs_dir, recurse=True)\n",
    "    print(f\"ğŸ§¹ Removed old thumbnails from '{thumbs_dir}'\")\n",
    "\n",
    "mssparkutils.fs.mkdirs(thumbs_dir)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“¤ Export Logic: Power BI Report Thumbnails (with timeout)\n",
    "# Exports first visible page of each report as a PNG file.\n",
    "# Retries are not implemented; failures are skipped with warnings.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def export_default_page_png(report_id: str, report_name: str) -> dict | None:\n",
    "    \"\"\"\n",
    "    Export first visible page of a Power BI report to PNG.\n",
    "    Aborts if export job does not complete in time.\n",
    "    Returns file metadata if successful, else None.\n",
    "    \"\"\"\n",
    "\n",
    "    # â”€â”€ Step 1: Fetch pages for the report\n",
    "    resp = requests.get(f\"{api}/reports/{report_id}/pages\", headers=headers)\n",
    "\n",
    "    if not resp.ok:\n",
    "        raise RuntimeError(\n",
    "            f\"âŒ Failed to retrieve pages for report '{report_name}' \"\n",
    "            f\"(HTTP {resp.status_code}): {resp.text}\"\n",
    "        )\n",
    "\n",
    "    pages = resp.json().get(\"value\", [])\n",
    "    visible_pages = sorted(\n",
    "        (p for p in pages if p.get(\"visibility\", \"Visible\") == \"Visible\"),\n",
    "        key=lambda p: p.get(\"order\", 0)\n",
    "    )\n",
    "\n",
    "    if not visible_pages:\n",
    "        raise RuntimeError(f\"âŒ No visible pages found for report '{report_name}'\")\n",
    "\n",
    "    page = visible_pages[0]\n",
    "    page_name = page[\"name\"]\n",
    "    page_display_name = page.get(\"displayName\", page_name)\n",
    "\n",
    "    print(f\"ğŸ“„ Exporting page â†’ '{page_display_name}' ({page_name})\")\n",
    "\n",
    "    # â”€â”€ Step 2: Start export job\n",
    "    export_resp = requests.post(\n",
    "        f\"{api}/reports/{report_id}/ExportTo\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"format\": \"PNG\",\n",
    "            \"powerBIReportConfiguration\": {\n",
    "                \"pages\": [{\"pageName\": page_name}]\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if not export_resp.ok:\n",
    "        raise RuntimeError(\n",
    "            f\"âŒ Failed to start export for report '{report_name}' \"\n",
    "            f\"(HTTP {export_resp.status_code}): {export_resp.text}\"\n",
    "        )\n",
    "\n",
    "    export_id = export_resp.json().get(\"id\")\n",
    "    start_ts = time.time()\n",
    "\n",
    "    # â”€â”€ Step 3: Poll until export succeeds, fails, or times out\n",
    "    while True:\n",
    "        status_resp = requests.get(\n",
    "            f\"{api}/reports/{report_id}/exports/{export_id}\",\n",
    "            headers=headers\n",
    "        )\n",
    "\n",
    "        if not status_resp.ok:\n",
    "            raise RuntimeError(\n",
    "                f\"âŒ Failed to poll export status for report '{report_name}' \"\n",
    "                f\"(HTTP {status_resp.status_code}): {status_resp.text}\"\n",
    "            )\n",
    "\n",
    "        status = status_resp.json().get(\"status\")\n",
    "\n",
    "        if status == \"Succeeded\":\n",
    "            break\n",
    "        if status in (\"Failed\", \"Cancelled\"):\n",
    "            print(f\"âŒ Export {status} for report '{report_name}' â€” skipped\")\n",
    "            return None\n",
    "        if time.time() - start_ts > MAX_EXPORT_SECONDS:\n",
    "            print(f\"âš ï¸ Export timed out after {MAX_EXPORT_SECONDS}s for report '{report_name}' â€” skipped\")\n",
    "            return None\n",
    "\n",
    "        time.sleep(POLL_INTERVAL_SECONDS)\n",
    "\n",
    "    # â”€â”€ Step 4: Download exported PNG file and move to Lakehouse\n",
    "    file_resp = requests.get(\n",
    "        f\"{api}/reports/{report_id}/exports/{export_id}/file\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if not file_resp.ok:\n",
    "        print(f\"âŒ Failed to download PNG for report '{report_name}' (HTTP {file_resp.status_code}) â€” skipped\")\n",
    "        return None\n",
    "    \n",
    "    file_bytes = file_resp.content\n",
    "\n",
    "    filename = f\"{report_id}.png\"\n",
    "    local_path = f\"/tmp/{filename}\"\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(file_bytes)\n",
    "\n",
    "    lakehouse_path = f\"{thumbs_dir}/{filename}\"\n",
    "    mssparkutils.fs.mv(\n",
    "        f\"file:{local_path}\",\n",
    "        lakehouse_path,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ’¾ Saved â†’ {lakehouse_path} ({len(file_bytes) / 1024:.1f} KB)\")\n",
    "\n",
    "    return {\n",
    "        \"ReportId\": report_id,\n",
    "        \"ReportName\": report_name,\n",
    "        \"FilePath\": f\"/lakehouse/default/{lakehouse_path}\",\n",
    "        \"FileSizeKB\": round(len(file_bytes) / 1024, 1)\n",
    "    }\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ” Process Reports (skip failed exports)\n",
    "# Attempt PNG export for each report and collect metadata.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "export_meta_rows = []\n",
    "\n",
    "for rep in reports:\n",
    "    print(f\"ğŸ“¤ Processing report '{rep['name']}' â€¦\")\n",
    "    result = export_default_page_png(rep[\"id\"], rep[\"name\"])\n",
    "    if result:\n",
    "        export_meta_rows.append(Row(**result))\n",
    "    else:\n",
    "        print(f\"âš ï¸ Thumbnail not created for report '{rep['name']}'\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“ Metadata Table\n",
    "# Write metadata about exported PNGs to Delta table.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "df_meta = spark.createDataFrame(export_meta_rows)\n",
    "\n",
    "df_meta.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"ReportMeta\")\n",
    "\n",
    "print(\"âœ… Complete. Table 'ReportMeta' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102ee4ee-a010-4524-ac1b-ee58c7decefd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Resize PNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117fcae2-816a-4fd0-90b0-76a73a2e14f1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“ Resize PNGs to Max 500px (Fabric-Safe, Binary-Preserving)\n",
    "# Resize thumbnails if their largest side exceeds 500px.\n",
    "# Results are safely overwritten in Lakehouse Files directory.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Read all PNG files from the thumbnails directory as binary content\n",
    "df_images = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.png\")\n",
    "    .load(\"Files/ReportThumbnails\")\n",
    "    .select(\"path\", \"content\")\n",
    ")\n",
    "\n",
    "# âš ï¸ Safe to collect â€” small number of thumbnails expected\n",
    "rows = df_images.collect()\n",
    "\n",
    "for row in rows:\n",
    "    full_path = row[\"path\"]\n",
    "    content = bytes(row[\"content\"])  # raw PNG bytes from Lakehouse\n",
    "\n",
    "    # â”€â”€ Open image in-memory using PIL\n",
    "    img = Image.open(io.BytesIO(content))\n",
    "    max_side = max(img.size)\n",
    "\n",
    "    if max_side <= 500:\n",
    "        # Skip if image is already within size constraints\n",
    "        print(f\"âœ… Already within limit â†’ {full_path}\")\n",
    "        continue\n",
    "\n",
    "    # â”€â”€ Resize image while preserving aspect ratio\n",
    "    scale = 500 / max_side\n",
    "    new_size = (\n",
    "        int(img.size[0] * scale),\n",
    "        int(img.size[1] * scale)\n",
    "    )\n",
    "\n",
    "    img = img.resize(new_size, Image.LANCZOS)\n",
    "\n",
    "    # â”€â”€ Save resized image to local /tmp path\n",
    "    filename = os.path.basename(full_path)\n",
    "    local_path = f\"/tmp/{filename}\"\n",
    "\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        img.save(f, format=\"PNG\")\n",
    "\n",
    "    # â”€â”€ Resolve Lakehouse-relative path from full Spark path\n",
    "    files_path = \"Files/\" + full_path.split(\"/Files/\")[1]\n",
    "\n",
    "    # â”€â”€ Move resized PNG back into Lakehouse (overwrite original)\n",
    "    mssparkutils.fs.mv(\n",
    "        f\"file:{local_path}\",\n",
    "        files_path,\n",
    "        overwrite=True\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“ Resized â†’ {files_path} ({new_size[0]}x{new_size[1]})\")\n",
    "\n",
    "print(\"âœ… Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782133d4-2106-4cf1-917b-4b6fe1a0e206",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save into the Lakehouse table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc93d52-60ba-476d-8350-b8d691000c87",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get metadata, save into Delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30ec5f-b52f-4950-850b-e7a10fa6ed4f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“„ Report + Refresh + Workspace Metadata (Explicit Schema, No Admin API)\n",
    "# Collects report-level metadata and last refresh status for owned datasets.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "meta_schema = T.StructType([\n",
    "    T.StructField(\"ReportId\", T.StringType(), False),\n",
    "    T.StructField(\"ReportName\", T.StringType(), True),\n",
    "    T.StructField(\"DatasetId\", T.StringType(), True),\n",
    "    T.StructField(\"WorkspaceId\", T.StringType(), True),\n",
    "    T.StructField(\"WorkspaceName\", T.StringType(), True),\n",
    "    T.StructField(\"WebUrl\", T.StringType(), True),\n",
    "    T.StructField(\"IsOwnedByMe\", T.BooleanType(), True),\n",
    "\n",
    "    # ğŸ” Dataset refresh metadata\n",
    "    T.StructField(\"LastRefreshStatus\", T.StringType(), True),\n",
    "    T.StructField(\"LastRefreshType\", T.StringType(), True),\n",
    "    T.StructField(\"LastRefreshError\", T.StringType(), True),\n",
    "\n",
    "    # ğŸ•’ TRUE timestamp columns\n",
    "    T.StructField(\"LastRefreshStartTime\", T.TimestampType(), True),\n",
    "    T.StructField(\"LastRefreshEndTime\", T.TimestampType(), True),\n",
    "])\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ›¡ï¸ ISO 8601 â†’ Python datetime parser (REQUIRED)\n",
    "# Handles timestamp conversion for Power BI API fields.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def parse_iso_timestamp(val):\n",
    "    \"\"\"Convert ISO 8601 string to datetime. Return None if invalid.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return datetime.fromisoformat(val.replace(\"Z\", \"+00:00\"))\n",
    "        except ValueError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¢ Resolve Workspace Names ONCE\n",
    "# Avoids repeated API calls by caching workspace display names.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "workspace_names = {}\n",
    "workspace_ids = {r[\"workspaceId\"] for r in reports if r.get(\"workspaceId\")}\n",
    "\n",
    "for ws_id in workspace_ids:\n",
    "    resp = requests.get(f\"{api}/groups/{ws_id}\", headers=headers)\n",
    "    workspace_names[ws_id] = resp.json().get(\"name\") if resp.ok else None\n",
    "\n",
    "print(f\"ğŸ¢ Workspace names resolved: {len(workspace_names)}\")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”„ Build Metadata Rows (STRICT TYPE CONTROL)\n",
    "# Collects refresh status and report info using per-report API calls.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "meta_rows = []\n",
    "\n",
    "for rep in reports:\n",
    "    report_id = rep[\"id\"]\n",
    "    report_name = rep[\"name\"]\n",
    "    workspace_id = rep.get(\"workspaceId\")\n",
    "\n",
    "    resp = requests.get(f\"{api}/reports/{report_id}\", headers=headers)\n",
    "    if not resp.ok:\n",
    "        raise RuntimeError(\n",
    "            f\"âŒ Failed to retrieve metadata for report '{report_name}' \"\n",
    "            f\"(HTTP {resp.status_code}): {resp.text}\"\n",
    "        )\n",
    "\n",
    "    details = resp.json()\n",
    "    dataset_id = details.get(\"datasetId\")\n",
    "\n",
    "    # Defaults\n",
    "    refresh_start = refresh_end = refresh_status = refresh_type = refresh_error = None\n",
    "\n",
    "    if dataset_id:\n",
    "        refresh_resp = requests.get(\n",
    "            f\"{api}/datasets/{dataset_id}/refreshes\",\n",
    "            headers=headers,\n",
    "            params={\"$top\": 1}\n",
    "        )\n",
    "\n",
    "        if refresh_resp.ok:\n",
    "            refreshes = refresh_resp.json().get(\"value\", [])\n",
    "            if refreshes:\n",
    "                r = refreshes[0]\n",
    "                refresh_start = parse_iso_timestamp(r.get(\"startTime\"))\n",
    "                refresh_end = parse_iso_timestamp(r.get(\"endTime\"))\n",
    "                refresh_status = r.get(\"status\")\n",
    "                refresh_type = r.get(\"refreshType\")\n",
    "                refresh_error = r.get(\"serviceExceptionJson\")\n",
    "\n",
    "    meta_rows.append(Row(\n",
    "        ReportId=report_id,\n",
    "        ReportName=report_name,\n",
    "        DatasetId=dataset_id,\n",
    "        WorkspaceId=workspace_id,\n",
    "        WorkspaceName=workspace_names.get(workspace_id),\n",
    "        WebUrl=details.get(\"webUrl\"),\n",
    "        IsOwnedByMe=details.get(\"isOwnedByMe\"),\n",
    "\n",
    "        LastRefreshStatus=refresh_status,\n",
    "        LastRefreshType=refresh_type,\n",
    "        LastRefreshError=refresh_error,\n",
    "\n",
    "        LastRefreshStartTime=refresh_start,\n",
    "        LastRefreshEndTime=refresh_end,\n",
    "    ))\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ’¾ Persist Metadata (NO CASTING NEEDED)\n",
    "# Saves enriched metadata as Delta table for downstream use.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "df_meta = spark.createDataFrame(meta_rows, schema=meta_schema)\n",
    "\n",
    "df_meta.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"ReportMeta\")\n",
    "\n",
    "print(\"âœ… ReportMeta written\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ee532-afd6-4c93-81d5-5a479919325c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Write base64 chunks of image data to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f764312-95a8-4125-bab6-2a356546fd81",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš™ï¸ Configuration\n",
    "# Chunk size (characters) for base64-encoded image data.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "CHUNK_SIZE = 8000\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ UDF: Binary â†’ Base64 â†’ Variable-Length Chunks\n",
    "# Converts PNG binary into base64 and splits it into only the chunks needed.\n",
    "# No padding and no artificial limits.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def bin_to_chunks(b: bytes):\n",
    "    if b is None:\n",
    "        return []\n",
    "\n",
    "    b64 = base64.b64encode(b).decode(\"utf-8\")\n",
    "\n",
    "    return [\n",
    "        b64[i:i + CHUNK_SIZE]\n",
    "        for i in range(0, len(b64), CHUNK_SIZE)\n",
    "    ]\n",
    "\n",
    "\n",
    "chunk_udf = F.udf(\n",
    "    bin_to_chunks,\n",
    "    T.ArrayType(T.StringType())\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“¥ Read PNG Files\n",
    "# Binary files are the source of truth for image content.\n",
    "# ReportId is extracted from the filename.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "df_files = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"pathGlobFilter\", \"*.png\")\n",
    "    .load(\"Files/ReportThumbnails\")\n",
    "    .select(\n",
    "        F.col(\"path\"),\n",
    "        F.col(\"content\").alias(\"ThumbnailBinary\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"ReportId\",\n",
    "        F.regexp_extract(\"path\", r\"/([^/]+)\\.png$\", 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”„ Binary â†’ Base64 â†’ Chunk Array\n",
    "# Each row contains an array sized exactly to the image payload.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "df_chunks_array = (\n",
    "    df_files\n",
    "    .withColumn(\"Chunks\", chunk_udf(F.col(\"ThumbnailBinary\")))\n",
    "    .select(\"ReportId\", \"Chunks\")\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”¢ Explode Chunks into Rows (FIXED)\n",
    "# posexplode returns (pos, col) and must be aliased explicitly.\n",
    "# ChunkIndex is 1-based for readability and stable ordering.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "df_chunks_rows = (\n",
    "    df_chunks_array\n",
    "    .filter(F.size(\"Chunks\") > 0)\n",
    "    .select(\n",
    "        \"ReportId\",\n",
    "        F.posexplode(\"Chunks\").alias(\"pos\", \"Chunk\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"ReportId\"),\n",
    "        (F.col(\"pos\") + F.lit(1)).alias(\"ChunkIndex\"),\n",
    "        F.col(\"Chunk\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ’¾ Write to ReportImages Table\n",
    "# Row-based storage avoids wide schemas and scales cleanly.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "(\n",
    "    df_chunks_rows.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .format(\"delta\")\n",
    "    .saveAsTable(\"ReportImages\")\n",
    ")\n",
    "\n",
    "print(\"âœ… Complete. Variable-length base64 chunks written to ReportImages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd7e435-a4b9-43f7-8403-a5c012638a58",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Final cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a44092-fd4f-4bb4-bbaa-9460a7a6c880",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf76a86-785a-442d-9e18-01f5b0ad3a78",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§¾ Post-Write Verification\n",
    "# Confirm database context and review table schemas.\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ“‚ Current database:\", spark.catalog.currentDatabase())\n",
    "\n",
    "# Review final schemas\n",
    "spark.table(\"ReportMeta\").printSchema()\n",
    "spark.table(\"ReportImages\").printSchema()\n",
    "\n",
    "print(\"âœ… Tables written successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4130ef4-f8bb-4fa7-baad-88ff0268398c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Possible improvements:\n",
    "\n",
    "split the notebbook in 2 notebooks (for metadata, and images), use different schedules (e.g., refresh metadata daily, images - weekly)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "29cba3da-ebac-46b3-b295-16b3d8758806",
    "default_lakehouse_name": "Lakehouse",
    "default_lakehouse_workspace_id": "5b129136-0c96-4457-88a3-89ed2ce65b04",
    "known_lakehouses": [
     {
      "id": "29cba3da-ebac-46b3-b295-16b3d8758806"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
